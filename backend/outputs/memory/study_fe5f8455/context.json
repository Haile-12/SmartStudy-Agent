{
  "session_id": "study_fe5f8455",
  "created_at": "2026-02-06T05:30:33.433929",
  "topic": "Design and analysis of algorithim",
  "notes": "Design and Analysis of\nAlgorithms\nIT3202 Students Course Note\nPrepared by Asmelash Girmay\nDepartment of Information Technology \nMekelle Institute of Technology\nMekelle University\nMekelle\nMarch 15, 2021\nChapter 1\nAnalysis of Algorithm\nAlgorithm: computational procedure that consists of a finite set of instructions by which, given an\ninput from some set of possible inputs, enable us to obtaining ab output through a systematic execution\nof instructions.\nAn algorithm  is a sequence of unambiguous instructions\nfor solving a problem, i.e., for obtaining a required output\nfor any legitimate input in a finite amount of time.\nAn  algorithm  can  be  implemented  in  more  than  one\nprogramming language.\nA problem can be solved in more than one ways. Hence,\nmany solution algorithms can be derived for a given problem.\nGiven computational problem, an algorithm describes a specific computational procedure for achieving\nthat input/output relationship. \nExample of algorithms:\n• Sorting – bubble sorting, insertion sorting, etc\n• Internet algorithms – routing algorithms, searching algorithms\n• Security algorithms – public key cryptography\n• Optimization algorithms – resources allocation \nExpressing algorithms:\n• flowchart \n• pseudo code \nHow do we choose between the different sorting algorithms?\n• We need to analyze them and choose the one with a better performance.\nAnalysis of Algorithms\nProcess of defining the performance of a program or algorithm \n• Performance: amount of computer memory and time needed to run a program of given algorithm\n• There are two methods to determine the performance of a program\n◦ A Priori Analysis  − This is a theoretical analysis of an algorithm that is performed before\nimplementation. Efficiency of an algorithm is measured by assuming that all other factors, for\nexample, processor speed, are constant and have no effect on the implementation.\n\n◦ A Posterior Analysis  − This is an empirical analysis of an algorithm that is performed after\nimplementation. The selected algorithm is implemented using programming language. This is\nthen executed on target computer machine. In this analysis, actual statistics like running time\nand space required, are collected.\nCharacteristics of an Algorithm\n• Unambiguous – Algorithm should be clear and unambiguous. Each of its steps (or phases), and\ntheir inputs/outputs should be clear and must lead to only one meaning.\n• Input − An algorithm should have 0 or more well-defined inputs.\n• Output − An algorithm should have 1 or more well-defined outputs, and should match the\ndesired output.\n• Finiteness − Algorithms must terminate after a finite number of steps.\n• Feasibility − Should be feasible with the available resources.\n• Independent − An algorithm should have step-by-step directions, which should be independent\nof any programming code.\nAlgorithm Complexity\nSuppose X is an algorithm and n is the size of input data, the time and space used by the algorithm X\nare the two main factors, which decide the efficiency of X.\n• Time  Factor −  Time  is  measured  by  counting  the  number  of  key  operations  such  as\ncomparisons in the algorithm.\n• Space Factor − Space is measured by counting the maximum memory space required by the\nalgorithm.\nThe complexity of an algorithm f(n) gives the running time and/or the storage space required by the\nalgorithm in terms of n as the size of input data.\nSpace complexity\nSpace complexity of an algorithm represents the amount of memory space required by the algorithm in\nits life cycle. The space required by an algorithm is equal to the sum of the following two components\n• A fixed part that is a space required to store certain data and variables, that are independent of\nthe size of the problem. For example, simple variables and constants used, program size, etc.\n• A variable part is a space required by variables, whose size depends on the size of the problem.\nFor example, dynamic memory allocation, recursion stack space, etc.\nSpace complexity S(P) of any algorithm P is S(P) = C + SP(I), where C is the fixed part and S(I) is the\nvariable part of the algorithm, which depends on instance characteristic I. Example: \nWe have three variables, A, B, C and one constant, 10.\nS(P) = C + SP(I) = 1 + 3 = 4.\nNow,  space  depends  on  data  types  of  given  variables  and\nconstant types and it will be multiplied accordingly.\n\nSpace complexity of a program – the amount of memory it needs to run to completion. A problem may\nhave several possible solutions with differing space requirements.\nMore generically, space needed by a program has the following components \na. Instruction space – space needed to store the compiled version of the a program instructions\nb. Data space – space needed to store all constants and variables values\nTwo components\n• space needed by constants and variables\n• space needed by competent variables such as array, structure, and dynamically allocated\nmemory\nc. environment stack space – stack is used to store information needed to resume execution of partially\ncompleted functions.\nTime Complexity\nTime complexity of an algorithm represents the amount of time required by the algorithm to run to\ncompletion.  Time  requirements  can  be  defined  as a  numerical  function  T(n),  where  T(n)  can  be\nmeasured as the number of steps, provided each step consumes constant time.\nFor example, addition of two n-bit integers takes n steps. Consequently, the total computational time is\nT(n) = c  n, where c is the time taken for the addition of two bits. Here, we observe that T(n) grows∗\nlinearly as the input size increases.\nASYMPTOTIC ANALYSIS\nAsymptotic  analysis  of  an  algorithm  refers  to  defining  the  mathematical  framing  of  its  run-time\nperformance. Using asymptotic analysis, we can very well conclude the best case, average case, and\nworst case scenario of an algorithm.\nUsually, the time required by an algorithm falls under three types\n• Best Case − Minimum time required for program execution.\n• Average Case − Average time required for program execution.\n• Worst Case − Maximum time required for program execution.\nAsymptotic Notations\nFollowing are the commonly used asymptotic notations to calculate the running time complexity of an\nalgorithm.\n• Ο Notation\n• Ω Notation\n• θ Notation\nBig Oh Notation, Ο\nThe notation Ο n is the formal way to express the upper bound of an\nalgorithm's running time. It measures the worst case time complexity\nor  the  longest  amount  of  time  an  algorithm  can  possibly  take  to\ncomplete. \nOmega Notation, Ω\nThe notation Ω n is the formal way to express the  lower bound  of an\nalgorithm's running time. It measures the best case time complexity or the\nbest amount of time an algorithm can possibly take to complete. \nTheta Notation, θ\nThe notation θ n is the formal way to express both the lower bound and\nthe upper bound of an algorithm's running time.\nAlgorithm Design Techniques/Strategies\n• Brute force\n• Divide and conquer\n• Decrease and conquer\n• Transform and conquer\n• Greedy approach\n• Dynamic programming\n• Backtracking and branch-and-bound\n• Space and time trade-offs\n• Iterative improvement\n• Branch and bound\n\nChapter 2\nDivide and Conquer\nIn divide and conquer approach, the problem in hand, is divided into smaller sub-problems and then\neach problem is solved independently. When we keep on dividing the sub-problems into even smaller\nsub-problems, we may eventually reach a stage where no more division is possible. Those \"atomic\"\nsmallest possible sub-problem (fractions) are solved. The solution of all sub-problems is finally merged\nin order to obtain the solution of an original problem.\nInspired by emperors and colonizers.\nIt has three steps:\n1. Divide the problem into smaller problems \n2. Conquer by solving problems in (1)\n3. Combine the results in (2) together.\nExample: Binary Search: Search for x in sorted array A. \nExamples: Binary Search, Merge sort, Quick sort, etc. Matrix multiplication, Selection, Convex Hulls.\nBinary Search\nLet T (n) denote the worst-case time to binary search in an array of length n. \nRecurrence is T (n) = T (n/2) + O(1). \nT (n) = O(log n).\nMerge Sort\nSort an unordered array of numbers A. \nLet T (n) denote the worst-case time to merge sort an array of\nlength n.\nRecurrence is T (n) = 2T (n/2) + O(n).\nT (n) = O(n log n).\n\nMerge Sort: Illustration\nMultiplying Numbers\nWe want to multiply two n-bit numbers. Cost is number of elementary bit steps. Grade school method \nhas Θ(n2 ) cost.:\nn2 multiplies, n2 /2 additions, plus some carries.\nDoesn’t hardware provide multiply? It is fast, optimized,\nand free. So, why bother?\n• True for numbers that fit in one computer word.\nBut what if numbers are very large.\n• Cryptography (encryption, digital signatures) uses\nbig number “keys.” Typically 256 to 1024 bits\nlong!\n• n2 multiplication too slow for such large numbers.\n• Karatsuba’s (1962) divide-and-conquer scheme multiplies two n bit numbers in O(n1.59 ) steps.\nKaratsuba’s Algorithm\nLet X and Y be two n-bit numbers. Write X = a b and Y = c d. Where a, b, c, d are n/2 bit numbers. \n(Assume n = 2k .). Hence,\nXY = (a2n/2 + b)(c2n/2 + d) = ac2n + (ad + bc)2n/2 + bd\nExample: X = 4729, Y = 1326\n• a = 47; b = 29 and c = 13; d = 26.\n• ac = 47  13 = 611; ad = 47  26 = 1222; bc = 29  13 = 377; and bd = 29  26 = 754∗ ∗ ∗ ∗\n\n• XY = 6110000 + 159900 + 754  = 6270654\nThis is D&C: Solve 4 problems, each of size n/2; then perform O(n) shifts to multiply the terms by 2n \nand 2n/2 . We can write the recurrence as T (n) = 4T (n/2) + O(n). But this solves to T (n) = O(n 2 )!\n• XY = ac2 n + (ad + bc)2 n/2 + bd.\n• Note that (a − b)(c − d) = (ac + bd) − (ad + bc).\n• Solve 3 subproblems: ac, bd, (a − b)(c − d).\nWe can get all the terms needed for XY by addition and subtraction! The recurrence for this algorithm \nis T (n) = 3T (n/2) + O(n) = O(n log2 3 ). The complexity is O(n log 2 3 ) ≈ O(n 1.59 ).\nThe recursion solution: review\nT (n) = 2T (n/2) + cn, with T (1) = 1.\nBy term expansion. \nSet i = log 2 n. Use T (1) = 1. We get T (n) = n + cn(log n) = O(n log n).\nThe Tree View\nT (n) = 2T (n/2) + cn, with T (1) = 1.\n# leaves = n; # levels = log n.\nWork per level is O(n), so total is O(n log n).\nSolving By Induction\nRecurrence: T (n) = 2T (n/2) + cn.\nBase case: T (1) = 1.\nClaim: T (n) = cn log n + cn.\nT (n) = 2T (n/2) + cn\n= 2 (c(n/2) log(n/2) + cn/2) + cn\n= cn (log n − 1 + 1) + cn\n= cn log n + cn\nMore Examples\nT (n) = 4T (n/2) + cn, T (1) = 1.\n• Stops when n/2 i = 1, and i = log n.\n• Recurrence solves to T (n) = O(n2 ).\n\nBy Term Expansion\nTerminates when 2 i = n, or i = log n.\n• 4 i = 2 i × 2 i = n × n = n 2 .\n• T (n) = n 2 + cn 2 = O(n 2 ).\nExercise\n1. T (n) = 2T (n/4) + √n, T (1) = 1.\nMaster Method\n• Number children multiply by factor a at \neach level. \n• Number of leaves is a log b n = n log b a . \n• Verify by taking logarithm on both sides.\n• By recursion tree, we get\n\nApplying Master Method\nExercise\n• Workout with the analysis of Matrix multiplication, Quick Sort, Linear time selection, Convex\nhulls\n\nChapter 3\nGraph Search, Shortest Paths, and Data Structures\nData Structures(heaps, balanced search trees, hash tables,bloom filters), graph primitives(applications\nof breadth-first and deep-first search, connectivity, shortest paths), and their applications(ranging from\nduplication to social network analysis)\nPlease refer to your data structures course materials.\nChapter 4\nGreedy Algorithms\nAn algorithm is designed to achieve optimum solution for a given problem. In greedy algorithm\napproach, decisions are made from the given solution domain. As being greedy, the closest solution that\nseems to provide an optimum solution is chosen.\nGreedy algorithms try to find a localized optimum solution, which may eventually lead to globally\noptimized  solutions.  However,  generally  greedy  algorithms  do  not  provide  globally  optimized\nsolutions.\nCounting Coins\nThis problem is to count to a desired value by choosing the least possible coins and the greedy\napproach forces the algorithm to pick the largest possible coin. If we are provided coins of $1, 2, 5 and\n10 and we are asked to count $ 18 then the greedy procedure will be −\n1. Select one $ 10 coin, the remaining count is 8\n2. Then select one $ 5 coin, the remaining count is 3\n3. Then select one $ 2 coin, the remaining count is 1\n4. And finally, the selection of one $ 1 coins solves the problem\nThough, it seems to be working fine, for this count we need to pick only 4 coins. But if we slightly\nchange the problem then the same approach may not be able to produce the same optimum result. For\nthe currency system, where we have coins of 1, 7, 10 value, counting coins for value 18 will be\nabsolutely optimum but for count like 15, it may use more coins than necessary. For example, the\ngreedy approach will use 10 + 1 + 1 + 1 + 1 + 1, total 6 coins. Whereas the same problem could be\nsolved by using only 3 coins 7 + 7 + 1. Hence, we may conclude that the greedy approach picks an\nimmediate optimized solution and may fail where global optimization is a major concern.\nExamples: Most networking algorithms use the greedy approach. Here is a list of few of them −\n• Travelling Salesman Problem\n• Prim's Minimal Spanning Tree Algorithm\n• Kruskal's Minimal Spanning Tree Algorithm\n• Dijkstra's Minimal Spanning Tree Algorithm\n• Graph - Map Coloring\n• Graph - Vertex Cover\n• Knapsack Problem\n• Job Scheduling Problem\nMinimum Spanning Trees and Prim’s Algorithm\nSpanning Trees: A sub-graph T of a undirected graph G (V , E) is a spanning tree of G if it is a tree and\ncontains every vertex of G. Example: \nEvery connected graph has a spanning tree. A weighted graph is a graph, in which each edge has a\nweight (some real number). Hence, weight of a Graph is the sum of the weights of all edges.\nA Minimum Spanning Tree in an undirected connected weighted graph is a spanning tree of minimum\nweight (among all spanning trees). Example: in the above graph, Tree 2 with w=71 is the MST.\nThe minimum spanning tree may not be unique. However, if the weights of all the edges are pairwise\ndistinct, it is indeed unique. Example:\n\nGeneric Algorithm for MST problem\nLet A be a set of edges such that A C T, where T is a MST. An edge (u, v) is a safe edge for A, if A U\n{(u,v)} is also a subset of some MST.\nIf at each step, we can find a safe edge (u, v), we can ’grow’ a MST. This leads to the following generic\napproach:\nGeneric-MST(G, w)\nLet A=EMPTY;\nwhile A does not form a spanning tree find an edge (u, v) that is safe for A add (u, v) to A\nreturn A\nHow can we find a safe edge?\nWe first give some definitions. Let G = (V , E) be a connected and undirected graph. We define:\nCut A cut (S, V – S) of G is a partition of V .\nCross An edge (u, v) E E crosses the cut (S, V – S) if one of its endpoints is in , and the other is in V - S.\nRespect A cut respects a set A of edges if no edge in A crosses the cut.\nLight edge An edge is a light edge crossing a cut if its weight is the minimum of any edge crossing the \ncut.\nLemma\nLet G = (V , E) be a connected, undirected graph with a real-valued weight function defined on E. Let A\nbe a subset of E that is included in some minimum spanning tree for G, let (S, V – S) be any cut of G\nthat respects A, and let (u, v) be a light edge crossing the cut (S, V – S). Then, edge (u, v) is safe for A.\nIt means that we can find a safe edge by:\n1. First finding a cut that respects A,\n2. Then finding the light edge crossing that cut.\nThat light edge is a safe edge.\nPrim’s Algorithm\nThe generic algorithm gives us an idea how to ’grow’ a MST. If you read the theorem and the proof\ncarefully, you will notice that the choice of a cut (and hence the corresponding light edge) in each\niteration is immaterial. We can select any cut (that respects the selected edges) and find the light edge\ncrossing that cut to proceed.\nThe Prim’s algorithm makes a nature choice of the cut in each iteration – it grows a single tree and adds\na light edge in each iteration.\nPrim’s Algorithm : How to grow a tree\n• Start by picking any vertex r to be the root of the tree.\n• While the tree does not contain all vertices in the graph find shortest edge leaving the tree and\nadd it to the tree .\nRunning time is O((|V| + |E|) log |V|).\nStep 0: Choose any element r; set S = {r} and A = {}. (Take r as the root of our spanning tree.)\nStep 1: Find a lightest edge such that one endpoint is in S and the other is in V \\ S. Add this edge to A\nand its (other) endpoint to S.\nStep 2: If V \\ S = {}, then stop and output (minimum) spanning tree (S, A). Otherwise go to Step 1. \nThe idea: expand the current tree by adding the lightest (shortest) edge leaving it and its endpoint.\nExercise: \nFind MST.\nAnswer: A={{a,b},{b,d},{c,d},{c,f},{f,g},{f,e}}\nShow the necessary steps.\nAnalysis of Prim’s Algorithm\n\nScheduling Classes\nThe next example is slightly less trivial. Suppose you decide to drop out of computer science at the last\nminute and change your major to Applied Chaos. The Applied Chaos department has all of its classes\non the same day every week, referred to as “Soberday\" by the students (but interestingly, not by the\nfaculty). Every class has a different start time and a different ending time: AC 101 (‘Toilet Paper\nLandscape  Architecture’)  starts  at  10:27pm  and  ends  at  11:51pm;  AC  666  (‘Immanentizing  the\nEschaton’) starts at 4:18pm and ends at 7:06pm, and so on. In the interests of graduating as quickly as\npossible, you want to register for as many classes as you can. (Applied Chaos classes don’t require any\nactual work.) The University’s registration computer won’t let you register for overlapping classes, and\nno one in the department knows how to override this ‘feature’. Which classes should you take?\nMore formally, suppose you are given two arrays S[1 .. n] and F [1 .. n] listing the start and finish times\nof each class. Your task is to choose the largest possible subset X  {1, 2, . . . , n} so that for any pair i,∈\nj  X , either S[i] > F [ j] or S[ j] > F [i]. We can illustrate the problem by drawing each class as a∈\nrectangle whose left and right x-coordinates show the start and finish times. The goal is to find a largest\nsubset of rectangles that do not overlap vertically.\nThis problem has a fairly simple recursive solution, based on the observation that either you take class\n1 or you don’t. Let B4 be the set of classes that end before class 1 starts, and let L8 be the set of classes\nthat start later than class 1 ends:\nB 4 = {i | 2 ≤ i ≤ n and F [i] < S[1]} L 8 = {i | 2 ≤ i ≤ n and S[i] > F [1]}\nIf class 1 is in the optimal schedule, then so are the optimal schedules for B4 and L8 , which we can\nfind recursively. If not, we can find the optimal schedule for {2, 3, . . . , n} recursively. So we should\ntry both choices and take whichever one gives the better schedule. Evaluating this recursive algorithm\n\nfrom the bottom up gives us a dynamic programming algorithm that runs in O(n 2 ) time. I won’t bother\nto go through the details, because we can do better.\nIntuitively, we’d like the first class to finish as early as possible, because that leaves us with the most\nremaining classes. If this greedy strategy works, it suggests the following very simple algorithm. Scan\nthrough the classes in order of finish time; whenever you encounter a class that doesn’t conflict with\nyour latest class so far, take it!\nWe can write the greedy algorithm somewhat more formally as follows. (Hopefully the first line is\nunderstandable.)\nThis algorithm clearly runs in O(n log n) time. To prove that this algorithm actually gives us a maximal\nconflict-free schedule, we use an exchange argument, similar to the one we used for tape sorting. We\nare not claiming that the greedy schedule is the only maximal schedule; there could be others. (See the\nfigures on the previous page.) All we can claim is that at least one of the maximal schedules is the one\nthat the greedy algorithm produces.\nLemma: At least one maximal conflict-free schedule includes the class that finishes first.\nTheorem: The greedy schedule is an optimal schedule.\n\nHuffman Codes\nA binary code assigns a string of 0s and 1s to each character in the alphabet. A binary code is prefix\nfree if no code is a prefix of any other. 7-bit ASCII and Unicode’s UTF-8 are both prefix-free binary\ncodes. Morse code is a binary code, but it is not prefix-free; for example, the code for S (· · ·) includes\nthe code for E (·) as a prefix. Any prefix-free binary code can be visualized as a binary tree with the\nencoded characters stored at the leaves. The code word for any symbol is given by the path from the\nroot to the corresponding leaf; 0 for left, 1 for right. The length of a codeword for a symbol is the depth\nof the corresponding leaf. (Note that the code tree is not a binary search tree. We don’t care at all about\nthe sorted order of symbols at the leaves. (In fact. the symbols may not have a well-defined order!)\nSuppose we want to encode messages in an n-character alphabet so that the encoded message is as\nshort as possible. Specifically, given an array frequency counts f [1 .. n], we want to compute a prefix\nfree binary code that minimizes the total encoded length of the message\n\nLet x and y be the two least frequent characters (breaking ties between equally frequent characters\narbitrarily). There is an optimal code tree in which x and y are siblings.\nHuffman codes are optimal prefix-free binary codes.\n\nExercises\n\n\n\nChapter 5\nDynamic Programming\nDynamic programming approach is similar to divide and conquer in breaking down the problem into\nsmaller and yet smaller possible sub-problems. But unlike, divide and conquer, these sub-problems are\nnot solved independently. Rather, results of these smaller sub-problems are remembered and used for\nsimilar or overlapping sub-problems.\nDynamic programming is used where we have problems, which can be divided into similar sub-\nproblems, so that their results can be re-used. Mostly, these algorithms are used for optimization.\nBefore solving the in-hand sub-problem, dynamic algorithm will try to examine the results of the\npreviously solved sub-problems. The solutions of sub-problems are combined in order to achieve the\nbest solution.\nSo we can say that −\n• The problem should be able to be divided into smaller overlapping sub-problem.\n• An optimum solution can be achieved by using an optimum solution of smaller sub-problems.\n• Dynamic algorithms use Memorization.\nComparison\nIn  contrast  to  greedy  algorithms,  where  local  optimization  is  addressed,  dynamic  algorithms  are\nmotivated for an overall optimization of the problem.\nIn contrast to divide and conquer algorithms, where solutions are combined to achieve an overall\nsolution, dynamic algorithms use the output of a smaller sub-problem and then try to optimize a bigger\nsub-problem. Dynamic algorithms use Memorization to remember the output of already solved sub-\nproblems.\nExample\nThe following computer problems can be solved using dynamic programming approach −\n• Fibonacci number series\n• Knapsack problem\n• Tower of Hanoi\n• All pair shortest path by Floyd-Warshall\n• Shortest path by Dijkstra\n• Project scheduling\nDynamic programming can be used in both top-down and bottom-up manner. And of course, most of\nthe times, referring to the previous solution output is cheaper than recomputing in terms of CPU cycles.\nComputing a Binomial Coefficient\nComputing a binomial coefficient is a standard example of applying dynamic programming to a non-\noptimization problem. You may recall from your studies of elementary combinatories that the binomial\ncoefficient, denoted C(n, k). \nAlgorithm: Binomial(n, k)\n What is the time efficiency of this algorithm?\n\nKnapsack\n\n\n\nEdit Distance \nThe edit distance between two words is the minimum number of letters, insertions, deletions, and\nsubstitutions required to transform one word to another.\nExample: to transform food to money\n=> Food → Mood → Mond → Mone → Money!\nCost  = 4 = 3 (substitutions) + 1 (insertion)\nGap representation F O O D\nM O N E Y\nOptional structure property:\nSuppose we have the gap representation for the shortest edit sequence of two strings. If we remove the\nlast column, the remaining columns must represent the shortest edit sequence for the remaining sub\nstrings. Because, if the sub-strings had a shortest edit sequence, we could just give the last column back\non to get a shortest edit sequence for the original string.\nGiven two strings A [1...m] and Bp1...n]\nEdit [i, j] =  smallest number of operations (insertion, deletion, substitution needed to change A[1...i] in\nto B[1...j].\nEdit = min {\nEdit (i-1, j-1) if Ai = Bj, \nEdit(i-1, j) + 1, \nEdit(i, j-1) + 1, \nEdit (i-1, j-1) + 1\n}\nDeletion: Last entry in the bottom row is empty, hence edit (i, j) ← 1 + edit (i-1, j)\nInsertion: Last entry in the top row is empty, hence edit (i, j) ← 1 + edit (i, j-1)\nSubstitution: Both rows have characters in their last column, hence \nedit (i, j) ← edit (i-1, j-) + (Ai != Bj), 0 – false, 1 – true.\nEdit (i, j) = j, if i = 0\n= i, if j = 0\n= min {\nEdit(i-1, j) + 1, \nEdit(i, j-1) + 1, \nEdit (i-1, j-1) + 1 if Ai != Bj, \nEdit (i-1, j-1) + 0 if Ai = Bj\n}\nThe edit distance between the original string S = Edit (m, n).\nExercise:\n1. Given that A:[MIT], B:[MU]. Find:\na) The three ways of replacing A with B. \nb) m\nc) n\nd) Edit (m, n).\nChapter 6\nShortest Paths Revisited, and NP-Complete Problems\nShortest paths (Bellman-ford, floyd warshall, Johnson), NP-completeness(what it means for algorithm\ndesigner, and strategies for coping with computationally intractable problems:analysis of heuristics,\nlocal search)\nPlease refer to the course materials provided and read more on the\naforementioned special topics.",
  "metadata": {
    "status": "initialized"
  }
}