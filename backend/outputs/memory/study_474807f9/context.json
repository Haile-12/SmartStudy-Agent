{
  "session_id": "study_474807f9",
  "created_at": "2026-02-06T05:05:31.943698",
  "topic": "Reinforcement learning",
  "notes": "Introduction to Reinforcement Learning\nChapter 4: Reinforcement Learning\nCourse Name: Machine Learning\nCourse Code: IT4210B\nTarget Audience: 4th Year IT Department Students\nDate: May 22, 2025\nAdapted with modifications from original slides by Dr. Paul Alexander Bilokon\nIntroduction to Reinforcement Learning\nA different kind of learning\nBranches of machine learning\nFrom David Silver:\n\nIntroduction to Reinforcement Learning\nA different kind of learning\nReinforcement learning is multidisciplinary\nFrom David Silver:\n\nIntroduction to Reinforcement Learning\nA different kind of learning\nReinforcement learning is not supervised machine learning\n▶ Reinforcement learning differs from other types of machine learning in that the\ntraining information is used to evaluate the actions rather than instruct as to what the\ncorrect actions should be.\n▶ Instructive feedback, as in supervised machine learning, points out the correct\naction to take independent of the action taken.\n▶ Evaluative feedback, as in reinforcement learning, points out how good the action\ntaken is, but not whether it is the best or the worst action possible.\n▶ This creates the need for active exploration, a trial-and-error search for good\nbehaviour.\nIntroduction to Reinforcement Learning\nA different kind of learning\nReinforcement learning is not unsupervised machine learning\n▶ One may be tempted to think of reinforcement learning as a kind of unsupervised\nmachine learning, because it does not rely on examples of correct behaviour.\n▶ However, reinforcement learning is concerned with maximising a reward signal rather\nthan trying to find hidden structure, as distinct from unsupervised machine learning.\nIntroduction to Reinforcement Learning\nElements of reinforcement learning\nAgent\nActions\nObservations\nAgent Environment\nAction: at\nState change: st+1\nReward: rt\nThe agent is the entity that takes actions.\nIntroduction to Reinforcement Learning\nElements of reinforcement learning\nEnvironment\nActions\nObservations\nAgent Environment\nAction: at\nState change: st+1\nReward: rt\nThe environment is the world in which the agent exists and operates.\nIntroduction to Reinforcement Learning\nElements of reinforcement learning\nAction\nActions\nObservations\nAgent Environment\nAction: at\nState change: st+1\nReward: rt\nThe action is a move made by the agent in the environment.\nIntroduction to Reinforcement Learning\nElements of reinforcement learning\nObservation\nActions\nObservations\nAgent Environment\nAction: at\nState change: st+1\nReward: rt\nThe observation provides the agent with information about the (possibly changed)\nenvironment after taking an action.\nIntroduction to Reinforcement Learning\nElements of reinforcement learning\nState\nActions\nObservations\nAgent Environment\nAction: at\nState change: st+1\nReward: rt\nThe state is a situation, which the agent perceives.\nIntroduction to Reinforcement Learning\nElements of reinforcement learning\nReward\nActions\nObservations\nAgent Environment\nAction: at\nState change: st+1\nReward: rt\nThe reward is the feedback that measures the success or failure of the agent’s action. It\ndefines the goal of a reinforcement learning problem.\nIntroduction to Reinforcement Learning\nElements of reinforcement learning\nTotal reward\nActions\nObservations\nAgent Environment\nAction: at\nState change: st+1\nReward: rt\nThe total (future) reward is given by Gt = ∑∞\ni=t+1 ri. May or may not converge.\nIntroduction to Reinforcement Learning\nElements of reinforcement learning\nDiscounted total reward\nActions\nObservations\nAgent Environment\nAction: at\nState change: st+1\nReward: rt\nThe discounted total reward is given by Gt = ∑∞\ni=t+1 γi−t−1ri, γ ∈ [0, 1] being the\ndiscount rate.\nIntroduction to Reinforcement Learning\nElements of reinforcement learning\nReward hypothesis\n▶ Reinforcement learning is based on the reward hypothesis:\nAll goals can be described by the maximisation of expected total reward.\nIntroduction to Reinforcement Learning\nElements of reinforcement learning\nHistory\nThe history consists in the sequence of all observations, actions, and rewards (i.e. all\nobservable variables) up to the current time:\nHt = s0, a0, r0, s1, a1, r1, s2, a2, r2, s3, a3, r3, . . . , st .\nIntroduction to Reinforcement Learning\nElements of reinforcement learning\nEnvironment state\n▶ The agent state, st, may or may not match the environment state, se\nt .\n▶ Consider for example, a poker game. The agent (a poker player) knows only his hand.\nThe environment state includes the hand of each poker player.\n▶ In chess, on the other hand, st = se\nt — it is a perfect information game.\nIntroduction to Reinforcement Learning\nElements of reinforcement learning\nMarkov state\n▶ A state is said to be Markov iff\nP [st+1 | st ] = P [st+1 | s0, . . . , st ] ,\nin other words, the future is independent of the past given the present.\nIntroduction to Reinforcement Learning\nElements of reinforcement learning\nPolicy\n▶ A policy is the agent’s behaviour.\n▶ It is a map from state to action.\n▶ Deterministic policy: a = π(s).\n▶ Stochastic policy: π(a | s) = P [At = a | St = s].\nIntroduction to Reinforcement Learning\nElements of reinforcement learning\nValue function\n▶ A value function is a prediction of future reward.\n▶ Used to evaluate the goodness/badness of states.\n▶ And therefore to select between actions, e.g.\nvπ(s) = Eπ[rt + γrt+1 + γ2rt+2 + γ3rt+3 + . . . | St = s].\n▶ Whereas the reward signal indicates what is good in an immediate sense, a value\nfunction specifies what is good in the long run.\n▶ Roughly speaking, the value of a state is the total amount of reward an agent can\nexpect to accumulate over the future, starting from that state.\nIntroduction to Reinforcement Learning\nElements of reinforcement learning\nModel\n▶ A model predicts what the environment will do next.\n▶ P predicts the next state.\n▶ R predicts the next (immediate) reward.\nIntroduction to Reinforcement Learning\nExamples of reinforcement learning\nPhil’s breakfast\nFrom [SB18], inspired by [Agr88]:\nPhil prepares his breakfast. Closely examined, even this apparently mundane ac-\ntivity reveals a complex web of conditional behaviour and interlocking goal-subgoal\nrelationships: walking to the cupboard, opening it, selecting a cereal box, then\nreaching for, grasping, and retrieving the box. Other complex, tuned, interactive\nsequences of behaviour are required to obtain a bowl, spoon, and milk carton.\nEach step involves a series of eye movements to obtain information and to guide\nreaching and locomotion. Rapid judgments are continually made about how to\ncarry the objects or whether it is better to ferry some of them to the dining table\nbefore obtaining others. Each step is guided by goals, such as grasping a spoon\nor getting to the refrigerator, and is in service of other goals, such as having the\nspoon to eat with once the cereal is prepared and ultimately obtaining nourish-\nment. Whether he is aware of it or not, Phil is accessing information about the\nstate of his body that determines his nutritional needs, level of hunger, and food\npreferences.\nIntroduction to Reinforcement Learning\nExamples of reinforcement learning\nA prop trader\nA proprietary trader [Car15, Cha08, Cha13, Cha16, Dur13, Tul15] observes the dynamics\nof market securities and watches economic releases and news unfold on his Bloomberg\nterminal. Based on this information, considering both the tactical and strategic information,\nhe places buy and sell orders, stop losses and stop gains. The trader’s goal is to have a\nstrong PnL.\nIntroduction to Reinforcement Learning\nExamples of reinforcement learning\nAn options market maker\nA vanilla options market maker [Che98, Cla10, JFB15, Tal96, Wys17] produces two-sided\nquotes in FX options. She hedges her options position with spot. The market moves all the\ntime, so her risk (delta, gamma, vega, etc.) keeps changing. The market maker’s goal is to\nhedge the position as safely and as cheaply as possible.\nIntroduction to Reinforcement Learning\nOrigins of reinforcement learning\nDonald Michie on trial and error (i)\nDonald Michie FRSE FBCS\nFrom the point of view of one of the players, any game,\nsuch as Tic-Tac-Toe, represents a sequential decision\nprocess. Sooner or later the sequence of choices ter-\nminates in an outcome, to which a value is attached,\naccording to whether the game has been won, drawn\nor lost. If the player is able to learn from experience,\nthe choices which have led up to a given outcome\nreceive reinforcements in the light of the outcome\nvalue. In general, positive outcomes are fed back in\nthe form of positive reinforcement, that is to say, the\nchoices belonging to the successful sequence become\nmore probable on later recurrence of the same situa-\ntions. Similarly, negative outcomes are fed back as\nnegative reinforcements. [Mic63]\nIntroduction to Reinforcement Learning\nOrigins of reinforcement learning\nDonald Michie on trial and error (ii)\nDonald Michie FRSE FBCS\nThis picture of trial-and-error learning uses the con-\ncepts and terminology of the experimental psycholo-\ngist. Observations on animals agree with common\nsense in suggesting that the strength of reinforcement\nbecomes less as we proceed backwards along the\nloop from the terminus towards the origin. The more\nrecent the choice in the sequence, the greater its prob-\nable share of responsibility for the outcome. This pro-\nvides an adequate conceptual basis for a trial-and-\nerror learning device, provided that the total number\nof choice-points which can be encountered is small\nenough for them to be individually listed. [Mic63]\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nCheckers (i)\nThe game of checkers [Sam59, Sam67], following some ideas from [Sha50].\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nCheckers (ii)\nIn Some Studies in Machine Learning Using the Game of Checkers [Sam59]:\nTwo machine-learning procedures have been investigated in some detail using the\ngame of checkers. Enough work has been done to verify the fact that a computer\ncan be programmed so that it will learn to play a better game of checkers than can\nbe played by the person who wrote the program. Furthermore, it can learn to do\nthis in a remarkably short period of time (8 or 10 hours of machine-playing time)\nwhen given only the rules of the game, a sense of direction, and a redundant and\nincomplete list of parameters which are thought to have something to do with the\ngame, but whose correct signs and relative weights are unknown and unspecified.\nThe principles of machine learning verified by these experiments are, of course,\napplicable to many other situations.\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nCheckers (iii)\nIn Some Studies in Machine Learning Using the Game of Checkers. II — Recent\nProgress [Sam67]:\nA new signature table technique is described together with an improved book\nlearning procedure which is thought to be much superior to the linear polynomial\nmethod described earlier. Full use is made of the so-called “alpha-beta” pruning\nand several forms of forward pruning to restrict the spread of the move tree and to\npermit the program to look ahead to a much greater depth than it otherwise could\ndo. While still unable to outplay checker masters, the program’s playing ability has\nbeen greatly improved.\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nBackgammon (i)\nThe game of backgammon [Tes92, Tes94, Tes95, Tes02].\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nBackgammon (ii)\nIn Practical Issues in Temporal Difference Learning[Tes92]:\nThis paper examines whether temporal difference methods for training connec-\ntionist networks, such as Sutton’s TD(λ) algorithm, can be successfully applied to\ncomplex real-world problems. A number of important practical issues are identified\nand discussed from a general theoretical perspective. These practical issues are\nthen examined in the context of a case study in which TD(λ) is applied to learning\nthe game of backgammon from the outcome of self-play. This is apparently the\nfirst application of this algorithm to a complex nontrivial task. It is found that, with\nzero knowledge built in, the network is able to learn from scratch to play the entire\ngame at a fairly strong intermediate level of performance, which is clearly better\nthan conventional commercial programs, and which in fact surpasses compara-\nble networks trained on a massive human expert data set. The hidden units in\nthese networks have apparently discovered useful features, a longstanding goal\nof computer games research. Furthermore, when a set of hand-crafted features is\nadded to the input representation, the resulting networks reach a near-expert level\nof performance, and have achieved good results against world-class human play.\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nBackgammon (iii)\nIn TD-Gammon, A Self-Teaching Backgammon Program, Achieves Master-Level\nPlay [Tes94]:\nTD-Gammon is a neural network that is able to teach itself to play backgam-\nmon solely by playing against itself and learning from the results based on the\nTD(λ) reinforcement learning algorithm [Sut88]. Despite starting from random\ninitial weights (and hence random initial strategy), TD-Gammon achieves a sur-\nprisingly strong level of play. With zero knowledge built in at the start of learning\n(i.e. given only a “raw” description of the board state), the network learns to play\nat a strong intermediate level. Furthermore, when a set of hand-crafted features\nis added to the network’s input representation, the result is a truly staggering level\nof performance: the latest version of TD-Gammon is now estimated to play at a\nstrong master level that is extremely close to the world’s best human players.\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nBackgammon (iv)\nIn Temporal Difference Learning with TD-Gammon [Tes95]:\nTD-Gammon is a neural network that is able to teach itself to play backgam-\nmon solely by playing against itself and learning from the results based on the\nTD(λ) reinforcement learning algorithm [Sut88]. Despite starting from random\ninitial weights (and hence random initial strategy), TD-Gammon achieves a sur-\nprisingly strong level of play. With zero knowledge built in at the start of learning\n(i.e. given only a “raw” description of the board state), the network learns to play\nat a strong intermediate level. Furthermore, when a set of hand-crafted features\nis added to the network’s input representation, the result is a truly staggering level\nof performance: the latest version of TD-Gammon is now estimated to play at a\nstrong master level that is extremely close to the world’s best human players.\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nGo (i)\nThe game of go [SHM+16, SSS+17].\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nGo (ii)\nIn Mastering the game of Go with deep neural networks and tree search [SHM+16]:\nThe game of Go has long been viewed as the most challenging of classic games\nfor artificial intelligence owing to its enormous search space and the difficulty of\nevaluating board positions and moves. Here we introduce a new approach to\ncomputer Go that uses ‘value networks’ to evaluate board positions and ‘policy\nnetworks’ to select moves. These deep neural networks are trained by a novel\ncombination of supervised learning from human expert games, and reinforcement\nlearning from games of self-play. Without any lookahead search, the neural net-\nworks play Go at the level of state-of-the-art Monte Carlo tree search programs that\nsimulate thousands of random games of self-play. We also introduce a new search\nalgorithm that combines Monte Carlo simulation with value and policy networks.\nUsing this search algorithm, our program AlphaGo achieved a 99.8% winning rate\nagainst other Go programs, and defeated the human European Go champion by 5\ngames to 0. This is the first time that a computer program has defeated a human\nprofessional player in the full-sized game of Go, a feat previously thought to be at\nleast a decade away.\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nGo (iii)\nIn Mastering the Game of Go without Human Knowledge [SSS+17]:\nA long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa,\nsuperhuman proficiency in challenging domains. Recently, AlphaGo became the\nfirst program to defeat a world champion in the game of Go. The tree search\nin AlphaGo evaluated positions and selected moves using deep neural networks.\nThese neural networks were trained by supervised learning from human expert\nmoves, and by reinforcement learning from self-play. Here, we introduce an al-\ngorithm based solely on reinforcement learning, without human data, guidance,\nor domain knowledge beyond game rules. AlphaGo becomes its own teacher: a\nneural network is trained to predict AlphaGo’s own move selections and also the\nwinner of AlphaGo’s games. This neural network improves the strength of tree\nsearch, resulting in higher quality move selection and stronger self-play in the next\niteration. Starting tabula rasa, our new program AlphaGo Zero achieved super-\nhuman performance, winning 100-0 against the previously published, champion-\ndefeating AlphaGo.\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nWatson’s Daily-Double wagering (i)\nThe game of Jeopardy! [TGL+12, TGL+13].\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nWatson’s Daily-Double wagering (ii)\nIn Simulation, learning, and optimization techniques in Watson’s game strategies[TGL+12]:\nThe game of Jeopardy! features four types of strategic decision-making: 1) Daily\nDouble wagering; 2) Final Jeopardy! wagering; 3) selecting the next square when\nin control of the board; and 4) deciding whether to attempt to answer, i.e., “buzz in”.\nStrategies that properly account for the game state and future event probabilities\ncan yield a huge boost in overall winning chances, when compared with simple\n“rule-of-thumb” strategies. In this paper, we present an approach to developing\nand testing components to make said strategy decisions, founded upon develop-\nment of reasonably faithful simulation models of the players and the Jeopardy!\ngame environment. We describe machine learning and Monte Carlo methods\nused in simulations to optimize the respective strategy algorithms. Application of\nthese methods yielded superhuman game strategies for IBM Watson that signifi-\ncantly enhanced its overall competitive record.\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nWatson’s Daily-Double wagering (iii)\nIn Analysis of Watson’s Strategies for Playing Jeopardy![TGL+13]:\nMajor advances in Question Answering technology were needed for IBM Watson\nto play Jeopardy! at championship level — the show requires rapid-fire answers to\nchallenging natural language questions, broad general knowledge, high precision,\nand accurate confidence estimates. In addition, Jeopardy! features four types of\ndecision making carrying great strategic importance: (1) Daily Double wagering;\n(2) Final Jeopardy wagering; (3) selecting the next square when in control of the\nboard; (4) deciding whether to attempt to answer, i.e. “buzz in.” Using sophisti-\ncated strategies for these decisions, that properly account for the game state and\nfuture event probabilities, can significantly boost a player’s overall chances to win,\nwhen compared with simple “rule of thumb” strategies.\nThis article presents our approach to developing Watson’s game-playing strate-\ngies comprising development of a faithful simulation model, and then using learn-\ning and Monte-Carlo methods within the simulator to optimise Watson’s strategic\ndecision-making. After giving a detailed description of each of our game-stragegy\nalgorithms, we then focus in particular on validating the accuracy of the simula-\ntor’s predictions, and documenting performance improvements using our methods.\nQuantitative performance benefits are shown with respect to both simple heuris-\ntic strategies, and actual human contestant performance in historical episodes.\nWe further extend our analysis of human play to derive a number of valuable and\ncounterintuitive examples illustrating how human contestants may improve their\nperformance on the show.\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nAtari games (i)\nAtari 2600 games, such as Breakout [MKS+13, MKS+15].\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nAtari games (ii)\nIn Playing Atari with Deep Reinforcement Learning [MKS+13]:\nWe present the first deep learning model to successfully learn control policies\ndirectly from high-dimensional sensory input using reinforcement learning. The\nmodel is a convolutional neural network, trained with a variant of Q-learning,\nwhose raw input is raw pixels and whose output is a value function estimating\nfuture rewards. We apply our method to seven Atari 2600 games from the Arcade\nLearning Environment, with no adjustment of the architecture or learning algo-\nrithm. We find that it outperforms all previous approaches on six of the games and\nsurpasses a human expert on three of them.\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nAtari games (iii)\nIn Human-level control through deep reinforcement learning [MKS+15]:\nThe theory of reinforcement learning provides a normative account deeply rooted in psycho-\nlogical and neuroscientific perspectives on animal behaviour, of how agents may optimize\ntheir control of an environment. To use reinforcement learning successfully in situations ap-\nproaching real-world complexity, however, agents are confronted with a difficult task: they must\nderive efficient representations of the environment from high-dimensional sensory inputs, and\nuse these to generalise past experience to new situations. Remarkably, humans and other an-\nimals seem to solve this problem through a harmonious combination of reinforcement learning\nand hierarchical sensory processing systems, the former evidenced by a wealth of neural data\nrevealing notable parallels between the phasic signals emitted by dopaminergic neurons and\ntemporal difference reinforcement learning algorithms. While reinforcement learning agents\nhave achieved some successes in a variety of domains, their applicability has previously been\nlimited to domains in which useful features can be handcrafted, or to domains with fully ob-\nserved, low-dimensional state spaces. Here we use recent advances in training deep neural\nnetworks to develop a novel artificial agent, termed a deep Q-network, that can learn suc-\ncessful policies directly from high-dimensional sensory inputs using end-to-end reinforcement\nlearning. We tested this agent on the challenging domain of classic Atari 2600 games. We\ndemonstrate that the deep Q-network agent, receiving only the pixels and the game score as\ninputs, was able to surpass the performance of all previous algorithms and achieve a level\ncomparable to that of a professional human games tester across a set of 49 games, using\nthe same algorithm, network architecture and hyperparameters. This work bridges the divide\nbetween high-dimensional sensory inputs and actions, resulting in the first artificial agent that\nis capable of learning to excel at a diverse array of challenging tasks.\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nPersonalised web services (i)\nPersonalised web services [TTG15, Tho15].\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nPersonalised web services (ii)\nIn [TTG15]:\nIn this paper, we propose a framework for using reinforcement learning (RL) algo-\nrithms to learn good policies for personalised ad recommendation (P AR) sys-\ntems. The RL algorithms take into account the long-term effect of an action, and\nthus, could be more suitable than myopic techniques like supervised learning and\ncontextual bandit, for modern PAR systems in which the number of returning visi-\ntors is rapidly growing. However, while myopic techniques have been well-studied\nin PAR systems, the RL approach is still in its infancy, mainly due to two fundamen-\ntal challenges: how to compute a good RL strategy and how to evaluate a solution\nusing historical data to ensure its “safety” before deployment. In this paper, we pro-\npose to use a family of off-policy evaluation techniques with statistical guarantees\nto tackle both these challenges. We apply these methods to a real PAR problem,\nboth for evaluating the final performance and for optimising the parameters of the\nRL algorithm. Our results show that a RL algorithm equipped with these off-policy\nevaluation techniques outperforms the myopic approaches. Our results also give\nfundamental insights on the difference between the click through rate (CTR) and\nlife-time value (LTV) metrics for evaluating the performance of a PAR algorithm.\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nCooling optimisation for data centres (i)\nCooling optimisation for data centres [LWTG19].\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nCooling optimisation for data centres (ii)\nIn Transforming Cooling Optimization for Green Data Centre via Deep Reinforcement\nLearning [LWTG19]:\nCooling system plays a critical role in a modern data centre (DC). Developing an optimal\ncontrol policy for DC cooling system is a challenging task. The prevailing approaches often\nrely on approximating system models that are built upon the knowledge of mechanical cooling,\nelectrical and thermal management, which is difficult to design and may lead to suboptimal or\nunstable performances. In this paper, we propose utilising the large amount of monitoring\ndata in DC to optimise the control policy. To do so, we cast the cooling control policy design\ninto an energy cost minimisation problem with temperature constraints, and tap it into the\nemerging deep reinforcement learning (DRL) framework. Specifically, we propose an end-\nto-end cooling control algorithm (CCA) that is based on the actor-critic framework and an\noff-policy offline version of the deep deterministic policy gradient (DDPG) algorithm. In the\nproposed CCA, an evaluation network is trained to predict an energy cost counter penalised\nby the cooling status of the DC room, and a policy network is trained to predict optimised\ncontrol settings when given the current load and weather information. The proposed algorithm\nis evaluated on the EnergyPlus simulation platform and on a real data trace collected from the\nNational Super Computing Centre (NSCC) of Singapore. Our results show that the proposed\nCCA can achieve about 11% cooling cost saving on the simulation platform compared with a\nmanually configured baseline control algorithm. In the trace-based study, we propose a de-\nunderestimation validation mechanism as we cannot directly test the algorithm on a real DC.\nEven though with DUE the results are conservative, we can still achieve about 15% cooling\nenergy saving on the NSCC data trace if we set the inlet temperature threshold at 26.6 degree\nCelsius.\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nOptimising memory control (i)\nOptimising memory control [˙IMMC08, M˙I09].\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nOptimising memory control (ii)\nIn Self-Optimizing Memory Controllers: A Reinforcement Learning Approach [˙IMMC08]:\nEfficiently utilising off-chip DRAM bandwidth is a critical issue in designing cost-\neffective, high-performance chip multiprocessors (CMPs). Conventional memory\ncontrollers deliver relatively low performance in part because they often employ\nfixed, rigid access scheduling policies designed for average-case application be-\nhaviour. As a result, they cannot learn and optimise the long-term performance\nimpact of their scheduling decisions, and cannot adapt their scheduling policies to\ndynamic workload behaviour.\nWe propose a new, self-optimising memory controller design that operates using\nthe principles of reinforcement learning (RL) to overcome these limitations. Our\nRL-based memory controller observes the system state and estimates the long-\nterm performance impact of each action it can take. In this way, the controller\nlearns to optimise its scheduling policy on the fly to maximise long-term perfor-\nmance. Our results show that an RL-based memory controller improves the per-\nformance of a set of parallel applications run on a 4-core CMP by 19% on average\n(up to 33%), and it improves DRAM bandwidth utilisation by 22% compared to a\nstate-of-the-art controller.\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nOptimising memory control (iii)\nIn Dynamic Multicore Resource Management: A Machine Learning Approach [M˙I09]:\nA machine learning approach to multicore resource management produces self-\noptimising on-chip hardware agents capable of learning, planning, and continu-\nously adapting to changing workload demands. This results in more efficient and\nflexible management of critical hardware resources at runtime.\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nPacket routing in dynamically changing networks (i)\nPacket routing in dynamically changing networks [BL93].\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nPacket routing in dynamically changing networks (ii)\nIn Packet Routing in Dynamically Changing Networks: A Reinforcement Learning\nApproach [BL93]:\nThis paper describes the Q-routing algorithm for packet routing, in which a rein-\nforcement learning module is embedded into each node of a switching network.\nOnly local communication is used by each node to keep accurate statistics on\nwhich routing decisions lead to minimal delivery times. In simple experiments in-\nvolving a 36-node, irregularly connected network, Q-routing proves superior to a\nnonadaptive algorithm based on precomputed shortest paths and is able to route\nefficiently even when critical aspects of the simulation, such as the network load,\nare allowed to vary dynamically. The paper concludes with a discussion of the\ntradeoff between discovering shortcuts and maintaining stable policies.\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nMobile robots (i)\nMobile robots [SK02].\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nMobile robots (ii)\nIn Effective Reinforcement Learning for Mobile Robots [SK02]:\nProgramming mobile robots can be a long, time-consuming process. Specifying\nthe low-level mapping from sensors to actuators is prone to programmer miscon-\nceptions, and debugging such a mapping can be tedious. The idea of having a\nrobot learn how to accomplish a task, rather than being told explicitly is an appeal-\ning one. It seems easier and much more intuitive for the programmer to specify\nwhat the robot should be doing, and to let it learn the fine details of how to do it. In\nthis paper, we introduce a framework for reinforcement learning on mobile robots\nand describe our experiments using it to learn simple tasks.\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nRobocup soccer (i)\nRobocup soccer [SSK05].\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nRobocup soccer (ii)\nIn Reinforcement learning for robocup soccer keepaway [SSK05]:\nRoboCup simulated soccer presents many challenges to reinforcement learning\nmethods, including a large state space, hidden and uncertain state, multiple in-\ndependent agents learning simultaneously, and long and variable delays in the\neffects of actions. We describe our application of episodic SMDP Sarsa (λ) with\nlinear tile-coding function approximation and variableλ to learning higher-level de-\ncisions in a keepaway subtask of RoboCup soccer. In keepaway, one team “the\nkeepers”, tries to keep control of the ball for as long as possible despite the efforts\nof “the takers”. The keepers learn individually when to hold the ball and when to\npass to a teammate. Our agents learned policies that significantly outperform a\nrange of benchmark policies. We demonstrate the generality of our approach by\napplying it to a number of task variations including different field sizes and different\nnumbers of players on each team.\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nSelf-driving cars (i)\nAutonomous driving [SSSS16].\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nSelf-driving cars (ii)\nIn Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving [SSSS16]:\nAutonomous driving is a multi-agent setting where the host vehicle must apply sophisticated\nnegotiation skills with other road users when overtaking, giving way, merging, taking left and\nright turns and while pushing ahead in unstructured urban roadways. Since there are many\npossible scenarios, manually tackling all possible cases will likely yield a too simplistic policy.\nMoreover, one must balance between unexpected behaviour of other drivers/pedestrians and\nat the same time not to be too defensive so that normal traffic flow is maintained.\nIn this paper we apply deep reinforcement learning to the problem of forming long term driving\nstrategies. We note that there are two major challenges that make autonomous driving differ-\nent from other robotic tasks. First, is the necessity for ensuring functional safety — something\nthat machine learning has difficulty with given that performance is optimised at the level of an\nexpectation over many instances. Second, the Markov Decision Process model often used\nin robotics is problematic in our case because of unpredictable behaviour of other agents in\nthis multi-agent scenario. We make three contributions in our work. First, we show how policy\ngradient iterations can be used, and the variance of the gradient estimation using stochastic\ngradient ascent can be minimised, without Markovian assumptions. Second, we decompose\nthe problem into a composition of a Policy for Desires (which is to be learned) and trajectory\nplanning with hard constraints (which is not learned). The goal of Desires is to enable com-\nfort of driving, while hard constraints guarantees the safety of driving. Third, we introduce\na hierarchical temporal abstraction we call an “Option Graph” with a gating mechanism that\nsignificantly reduces the effective horizon and thereby reducing the variance of the gradient\nestimation even further. The Option Graph plays a similar role to “structured prediction” in\nsupervised learning, thereby reducing sample complexity, while also playing a similar role to\nLSTM gating mechanisms used in supervised deep networks.\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nThermal soaring (i)\nThermal soaring [RCSV16, WDV14].\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nThermal soaring (ii)\nIn Learning to soar in turbulent environments [RCSV16]:\nBirds and gliders exploit warm, rising atmospheric currents (thermals) to reach\nheights comparable to low-lying clouds with a reduced expenditure of energy. This\nstrategy of flight (thermal soaring) is frequently used by migratory birds. Soar-\ning provides a remarkable instance of complex decision making in biology and\nrequires a long-term strategy to effectively use the ascending thermals. Further-\nmore, the problem is technologically relevant to extend the flying range of au-\ntonomous gliders. Thermal soaring is commonly observed in the atomspheric\nconvective boundary layer on warm, sunny days. The formation of thermals un-\navoidably generates strong turbulent fluctuations, which constitute an essential\nelement of soaring. Here, we approach soaring flight as a problem of learning\nto navigate complex, highly fluctuating turbulent environments. We simulate the\natmospheric boundary layer by numerical models of turbulent convective flow and\ncombine them with model-free, experience-based, reinforcement learning algo-\nrithms to train the gliders. For the learned policies in the regimes of moderate and\nstrong turbulence levels, the glider adopts an increasingly conservative policy as\nturbulence levels increase, quantifying the degree of risk affordable in turbulent en-\nvironments. Reinforcement learning uncovers those sensorimotor cues that permit\neffective control over soaring in turbulent environments.\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nAutonomous helicopter flight (i)\nAutonomous helicopter flight [NCD+06].\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nAutonomous helicopter flight (ii)\nIn Autonomous helicopter flight via reinforcement learning [NKJS03]:\nAutonomous helicopter flight represents a challenging control problem, with com-\nplex, noisy, dynamics. In this paper, we describe a successful application of rein-\nforcement learning to autonomous helicopter flight. We first fit a stochastic nonlin-\near model of the helicopter dynamics. We then use the model to learn to hover in\nplace, and to fly a number of maneuvers taken from an RC helicopter competition.\nIntroduction to Reinforcement Learning\nSuccesses of reinforcement learning\nAutonomous helicopter flight (iii)\nIn Autonomous inverted helicopter flight via reinforcement learning [NCD+06]:\nHelicopters have highly stochastic, nonlinear, dynamics, and autonomous heli-\ncopter flight is widely regarded to be a challenging control problem. As helicopters\nare highly unstable at low speeds, it is particularly difficult to design controllers for\nlow speed aerobatic maneuvers. In this paper, we describe a successful applica-\ntion of reinforcement learning to designing a controller for sustained inverted flight\non an autonomous helicopter. Using data collected from the helicopter in flight,\nwe began by learning a stochastic, nonlinear model of the helicopter’s dynam-\nics. Then, a reinforcement learning algorithm was applied to automatically learn\na controller for autonomous inverted hovering. Finally, the resulting controller was\nsuccessfully tested on our autonomous helicopter platform.\nIntroduction to Reinforcement Learning\nFinancial applications of reinforcement learning\nFinancial applications of reinforcement learning\n\nIntroduction to Reinforcement Learning\nFinancial applications of reinforcement learning\nReinforcement Learning in finance—Kolm/Ritter\nIn Modern Perspectives on Reinforcement Learning in Finance [KR19b]:\nWe give an overview and outlook of the field of reinforcement learning as it ap-\nplies to solving financial applications of intertemporal choice. In finance, common\nproblems of this kind include pricing and hedging of contingent claims, investment\nand portfolio allocation, buying and selling a portfolio of securities subject to trans-\naction costs, market making, asset liability management and optimization of tax\nconsequences, to name a few. Reinforcement learning allows us to solve these\ndynamic optimization problems in an almost model-free way, relaxing the assump-\ntions often needed for classical approaches.\nA main contribution of this article is the elucidation of the link between these dy-\nnamic optimization problems and reinforcement learning, concretely addressing\nhow to formulate expected intertemporal utility maximization problems using mod-\nern machine learning techniques.\nIntroduction to Reinforcement Learning\nFinancial applications of reinforcement learning\nRL pricing and hedging—Halperin (i)\nIn QLBS: Q-Learner in the Black–Scholes (–Merton) Worlds [Hal17]:\nThis paper presents a discrete-time option pricing model that is rooted in Rein-\nforcement Learning (RL), and more specifically in the famous Q-Learning method\nof RL. We construct a risk-adjusted Markov Decision Process for a discrete-time\nversion of the classical Black-Scholes-Merton (BSM) model, where the option\nprice is an optimal Q-function, while the optimal hedge is a second argument of\nthis optimal Q-function, so that both the price and hedge are parts of the same\nformula. Pricing is done by learning to dynamically optimize risk-adjusted returns\nfor an option replicating portfolio, as in the Markowitz portfolio theory. Using Q-\nLearning and related methods, once created in a parametric setting, the model is\nable to go model-free and learn to price and hedge an option directly from data,\nand without an explicit model of the world. This suggests that RL may provide\nefficient data-driven and model-free methods for optimal pricing and hedging of\noptions, once we depart from the academic continuous-time limit, and vice versa,\noption pricing methods developed in Mathematical Finance may be viewed as spe-\ncial cases of model-based Reinforcement Learning. Further, due to simplicity and\ntractability of our model which only needs basic linear algebra (plus Monte Carlo\nsimulation, if we work with synthetic data), and its close relation to the original\nBSM model, we suggest that our model could be used for benchmarking of differ-\nent RL algorithms for financial trading applications.\nIntroduction to Reinforcement Learning\nFinancial applications of reinforcement learning\nRL pricing and hedging—Halperin (ii)\nIn The QLBS Q-Learner Goes NuQLear: Fitted Q Iteration, Inverse RL, and Option\nPortfolios [Hal18]:\nThe QLBS model is a discrete-time option hedging and pricing model that is based\non Dynamic Programming (DP) and Reinforcement Learning (RL). It combines the\nfamous Q-Learning method for RL with the Black–Scholes (–Merton) model’s idea\nof reducing the problem of option pricing and hedging to the problem of optimal\nrebalancing of a dynamic replicating portfolio for the option, which is made of a\nstock and cash.\nHere we expand on several NuQLear (Numerical Q-Learning) topics with the\nQLBS model. First, we investigate the performance of Fitted Q Iteration for a RL\n(data-driven) solution to the model, and benchmark it versus a DP (model-based)\nsolution, as well as versus the BSM model.\nSecond, we develop an Inverse Reinforcement Learning (IRL) setting for the\nmodel, where we only observe prices and actions (re-hedges) taken by a trader,\nbut not rewards.\nThird, we outline how the QLBS model can be used for pricing portfolios of options,\nrather than a single option in isolation, thus providing its own, data-driven and\nmodel independent solution to the (in)famous volatility smile problem of the Black–\nScholes model.\nIntroduction to Reinforcement Learning\nFinancial applications of reinforcement learning\nRL hedging—Kolm/Ritter\nIn Dynamic Replication and Hedging: A Reinforcement Learning Approach [KR19a]:\nThe authors of this article address the problem of how to optimally hedge an op-\ntions book in a practical setting, where trading decisions are discrete and trad-\ning costs can be nonlinear and difficult to model. Based on reinforcement learn-\ning (RL), a well-established machine learning technique, the authors propose a\nmodel that is flexible, accurate and very promising for real-world applications. A\nkey strength of the RL approach is that it does not make any assumptions about\nthe form of trading cost. RL learns the minimum variance hedge subject to what-\never transaction cost function one provides. All that it needs is a good simulator,\nin which transaction costs and option prices are simulated accurately.\nIntroduction to Reinforcement Learning\nFinancial applications of reinforcement learning\nDeep hedging—Buehler/Gonon/Teichmann/Wood/Mohan/Kochems\nIn Deep Hedging: Hedging Derivatives Under Generic Market Frictions Using\nReinforcement Learning [BGT+19]:\nThis article discusses a new application of reinforcement learning: to the problem\nof hedging a portfolio of “over-the-counter” derivatives under market frictions such\nas trading costs and liquidity constraints.\nThe objective is to maximise a non-linear risk-adjusted return function by trading\nin liquid hedging instruments such as equities or listed options. The approach\npresented here is the first efficient and model-independent algorithm which can be\nused for such problems at scale.\nIntroduction to Reinforcement Learning\nFinancial applications of reinforcement learning\nDeep hedging—Cao/Chen/Hull/Poulos\nIn Deep Hedging of Derivatives Using Reinforcement Learning [CCHZ19]:\nThis paper shows how reinforcement learning can be used to derive optimal hedg-\ning strategies for derivatives when there are transaction costs. The paper illus-\ntrates the approach by showing the difference between using delta hedging and\noptimal hedging for a short position in a call option when the objective is to mini-\nmize a function equal to the mean hedging cost plus a constant times the standard\ndeviation of the hedging cost. Two situations are considered. In the first, the asset\nprice follows geometric Brownian motion. In the second, the asset price follows a\nstochastic volatility process. The paper extends the basic reinforcement learning\napproach in a number of ways. First, it uses two different Q-functions so that both\nthe expected value of the cost and the expected value of the square of the cost are\ntracked for different state/action combinations. This approach increases the range\nof objective functions that can be used. Second, it uses a learning algorithm that\nallows for continuous state and action space. Third, it compares the accounting\nP&L approach (where the hedged position is valued at each step) and the cash\nflow approach (where cash inflows and outflows are used). We find that a hybrid\napproach involving the use of an accounting P&L approach that incorporates a\nrelatively simple valuation model works well. The valuation model does not have\nto correspond to the process assumed for the underlying asset price.\nIntroduction to Reinforcement Learning\nFinancial applications of reinforcement learning\nWealth management—Dixon/Halperin\nIn G-Learner and GIRL: Goal Based Wealth Management with Reinforcement\nLearning [DH20]:\nWe present a reinforcement learning approach to goal based wealth management problems\nsuch as optimization of retirement plans or target dated funds. In such problems, an investor\nseeks to achieve a financial goal by making periodic investments in the portfolio while being\nemployed, and periodically draws from the account when in retirement, in addition to the ability\nto re-balance the portfolio by selling and buying different assets (e.g. stocks). Instead of\nrelying on a utility of consumption, we present G-Learner: a reinforcement learning algorithm\nthat operates with explicitly defined one-step rewards, does not assume a data generation\nprocess, and is suitable for noisy data. Our approach is based on G-learning—a probabilistic\nextension of the Q-learning method of reinforcement learning.\nIn this paper, we demonstrate how G-learning, when applied to a quadratic reward and Gaus-\nsian reference policy, gives an entropy-regulated Linear Quadratic Regulator (LQR). This crit-\nical insight provides a novel and computationally tractable tool for wealth management tasks\nwhich scales to high dimensional portfolios. In addition to the solution of the direct problem of\nG-learning, we also present a new algorithm, GIRL, that extends our goal-based G-learning\napproach to the setting of Inverse Reinforcement Learning (IRL) where rewards collected by\nthe agent are not observed, and should instead be inferred. We demonstrate that GIRL can\nsuccessfully learn the reward parameters of a G-Learner agent and thus imitate its behavior.\nFinally, we discuss potential applications of the G-Learner and GIRL algorithms for wealth\nmanagement and robo-advising.\nIntroduction to Reinforcement Learning\nFinancial applications of reinforcement learning\nOptimal execution—Ning/Lin/Jaimungal\nIn Double Deep Q-Learning for Optimal Execution [NLJ18]:\nOptimal trade execution is an important problem faced by essentially all traders.\nMuch research into optimal execution uses stringent model assumptions and ap-\nplies continuous time stochastic control to solve them. Here, we instead take a\nmodel free approach and develop a variation of Deep Q-Learning to estimate the\noptimal actions of a trader. The model is a fully connected Neural Network trained\nusing Experience Replay and Double DQN with input features given by the cur-\nrent state of the limit order book, other trading signals, and available execution\nactions, while the output is the Q-value function estimating the future rewards un-\nder an arbitrary action. We apply our model to nine different stocks and find that\nit outperforms the standard benchmark approach on most stocks using the mea-\nsures of (i) mean and median out-performance, (ii) probability out-performance,\nand (iii) gain-loss ratios.\nIntroduction to Reinforcement Learning\nFinancial applications of reinforcement learning\nOptimal order placement—Schnaubelt\nIn Deep reinforcement learning for the optimal placement of cryptocurrency limit\norders [Sch20]:\nThis paper presents the first large-scale application of deep reinforcement learning\nto optimize the placement of limit orders at cryptocurrency exchanges. For train-\ning and out-of-sample evaluation, we use a virtual limit order exchange to reward\nagents according to the realized shortfall over a series of time steps. Based on\nthe literature, we generate features that inform the agent about the current mar-\nket state. Leveraging 18 months of high-frequency data with 300 million historic\ntrades and more than 3.5 million order book states from major exchanges and cur-\nrency pairs, we empirically compare state-or-the-art deep reinforcement learning\nalgorithms to several benchmarks. We find proximal policy optimization to reli-\nably learn superior order placement strategies when compared to deep double\nQ-networks and other benchmarks. Further analyses shed light into the black box\nof the learned execution strategy. Important features are current liquidity costs and\nqueue imbalances, where the latter can be interpreted as predictors of short-term\nmid-price returns. To preferably execute volume in limit orders to avoid additional\nmarket order exchange fees, order placement tends to be more aggressive in ex-\npectation of unfavorable price movements.",
  "metadata": {
    "status": "initialized"
  }
}