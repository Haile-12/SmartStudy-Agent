{
  "session_id": "study_1453c7c5",
  "created_at": "2026-02-06T06:58:49.743631",
  "topic": "Perceptron algorithm",
  "notes": "15-859(B) Machine Learning Theory\nAvrim Blum Lecture 4: January 25, 2010\nOnline Learning contd\n* The Perceptron Algorithm\n* Perceptron for Approximately Maximizing the Margins\n* Kernel Functions\nPlan for today: Last time we looked at the Winnow algorithm, which has a very n ice\nmistake-bound for learning an OR-function, which we then ge neralized for learning a linear\nseparator (technically we only did the extension to “ k of r” functions in class, but on home-\nwork 2 you will do the full analysis for general linear separa tors). Today will look at a more\nclassic algorithm for learning linear separators, with a di ﬀerent kind of guarantee.\n1 The Perceptron Algorithm\nOne of the oldest algorithms used in machine learning (from e arly 60s) is an online algorithm\nfor learning a linear threshold function called the Percept ron Algorithm.\nFor simplicity, we’ll use a threshold of 0, so we’re looking a t learning functions like:\nw1x1 + w2x2 + ... + wnxn > 0.\nWe can simulate a nonzero threshold with a “dummy” input x0 that is always 1, so this can\nbe done without loss of generality. The guarantee we’ll show for the Perceptron Algorithm\nis the following:\nTheorem 1 Let S be a sequence of labeled examples consistent with a linear th reshold func-\ntion w∗ · x > 0, where w∗ is a unit-length vector. Then the number of mistakes M on S\nmade by the online Perceptron algorithm is at most (1/γ )2, where\nγ = min\nx∈S\n|w∗ · x|\n||x|| .\n(I.e., if we scale examples to have Euclidean length 1, then γ is the minimum distance of\nany example to the plane w∗ · x = 0.)\nThe parameter “ γ ” is often called the “margin” of w∗ (or more formally, the L2 margin\nbecause we are scaling by the L2 lengths of the target and examples). Another way to view\nthe quantity w∗ · x/ ||x|| is that it is the cosine of the angle between x and w∗, so we will\nalso use cos( w∗, x) for it.\n1\nThe Perceptron Algorithm:\n1. Start with the all-zeroes weight vector w1 = 0, and initialize t to 1. Also let’s auto-\nmatically scale all examples x to have (Euclidean) length 1, since this doesn’t aﬀect\nwhich side of the plane they are on.\n2. Given example x, predict positive iﬀ wt · x > 0.\n3. On a mistake, update as follows:\n• Mistake on positive: wt+1 ← wt + x.\n• Mistake on negative: wt+1 ← wt − x.\nt ← t + 1.\nSo, this seems reasonable. If we make a mistake on a positive x we get wt+1·x = (wt+x)·x =\nwt ·x+1, and similarly if we make a mistake on a negative x we have wt+1 ·x = (wt −x)·x =\nwt · x − 1. So, in both cases we move closer (by 1) to the value we wanted .\nProof of Theorem 1 . We’re going to look at the magic quantities wt · w∗ and ||wt||.\nClaim 1: wt+1 · w∗ ≥ wt · w∗ + γ . That is, every time we make a mistake, the dot-product\nof our weight vector with the target increases by at least γ .\nProof: if x was a positive example, then we get wt+1 · w∗ = ( wt + x) · w∗ =\nwt · w∗ + x · w∗ ≥ wt · w∗ + γ (by deﬁnition of γ ). Similarly, if x was a negative\nexample, we get ( wt − x) · w∗ = wt · w∗ − x · w∗ ≥ wt · w∗ + γ .\nClaim 2: ||wt+1||2 ≤ ||wt||2 + 1. That is, every time we make a mistake, the length squared\nof our weight vector increases by at most 1.\nProof: if x was a positive example, we get ||wt + x||2 = ||wt||2 + 2wt · x + ||x||2.\nThis is less than ||wt||2 + 1 because wt · x is negative (remember, we made a\nmistake on x). Same thing (ﬂipping signs) if x was negative but we predicted\npositive.\nClaim 1 implies that after M mistakes, wM +1 · w∗ ≥ γM . On the other hand, Claim 2\nimplies that after M mistakes, ||wM +1|| ≤\n√\nM . Now, all we need to do is use the fact that\nwt · w∗ ≤ || wt||, since w∗ is a unit vector. So, this means we must have γM ≤\n√\nM , and\nthus M ≤ 1/γ 2.\nDiscussion: In the worst case, γ can be exponentially small in n. On the other hand, if we’re\nlucky and the data is well-separated, γ might even be large compared to 1 /n . This is called\nthe “large margin” case. (In fact, the latter is the more mode rn spin on things: namely, that\nin many natural cases, we would hope that there exists a large -margin separator.) In fact,\none nice thing here is that the mistake-bound depends on just a purely geometric quantity:\n2\nthe amount of “wiggle-room” available for a solution and doe sn’t depend in any direct way\non the number of features in the space.\nSo, if data is separable by a large margin, then Perceptron is a good algorithm to use.\nWhat if there is no perfect separator? What if only most of the data is separable by\na large margin, or what if w∗ is not perfect? We can see that the thing we need to look at\nis Claim 1. Claim 1 said that we make “ γ amount of progress” on every mistake. Now it’s\npossible there will be mistakes where we make very little pro gress, or even negative progress.\nOne thing we can do is bound the total number of mistakes we mak e in terms of the total\ndistance we would have to move the points to make them actuall y separable by margin γ .\nLet’s call that TD γ. Then, we get that after M mistakes, wM +1 · w∗ ≥ γM − TDγ. So,\ncombining with Claim 2, we get that\n√\nM ≥ γM − TDγ. We could solve the quadratic, but\nthis implies, for instance, that M ≤ 1/γ 2 + (2/γ )TDγ. The quantity 1\nγ TDγ is called the total\nhinge-loss of w∗.\nSo, this is not too bad: we can’t necessarily say that we’re ma king only a small multiple of\nthe number of mistakes that w∗ is (in fact, the problem of ﬁnding an approximately-optimal\nseparator is NP-hard), but we can say we’re doing well in term s of the “total distance”\nparameter.\nPerceptron for approximately maximizing margins. We saw that the perceptron\nalgorithm makes at most 1/γ 2 mistakes on any sequence of examples that is linearly-separ able\nby margin γ (i.e., any sequence for which there exists a unit-length vec tor w∗ such that all\nexamples x satisfy ℓ(x)(w∗ · x)/ ||x|| ≥ γ , where ℓ(x) ∈ {−1, 1} is the label of x).\nSuppose we are handed a set of examples S and we want to actually ﬁnd a large-margin\nseparator for them. One approach is to directly solve for the maximum-margin separator\nusing convex programming (which is what is done in the SVM alg orithm). However, if we\nonly need to approximately maximize the margin, then another approach is to use Percept ron.\nIn particular, suppose we cycle through the data using the Pe rceptron algorithm, updating\nnot only on mistakes, but also on examples x that our current hypothesis gets correct by\nmargin less than γ/ 2. Assuming our data is separable by margin γ , then we can show that\nthis is guaranteed to halt in a number of rounds that is polyno mial in 1 /γ . (In fact, we can\nreplace γ/ 2 with (1 − ǫ)γ and have bounds that are polynomial in 1 / (ǫγ ).)\nThe Margin Perceptron Algorithm (γ ):\n1. Assume again that all examples are normalized to have Eucl idean length 1. Initialize\nw1 = ℓ(x)x, where x is the ﬁrst example seen and initialize t to 1.\n2. Predict positive if wt·x\n||wt|| ≥ γ/ 2, predict negative if wt·x\n||wt|| ≤ − γ/ 2, and consider an\nexample to be a margin mistake when wt·x\n||wt|| ∈ (−γ/ 2, γ/ 2).\n3. On a mistake (incorrect prediction or margin mistake), up date as in the standard\nPerceptron algorithm: wt+1 ← wt + ℓ(x)x; t ← t + 1.\nTheorem 2 Let S be a sequence of labeled examples consistent with a linear th reshold func-\n3\ntion w∗ · x > 0, where w∗ is a unit-length vector, and let\nγ = min\nx∈S\n|w∗ · x|\n||x|| .\nThen the number of mistakes (including margin mistakes) mad e by Margin Perceptron (γ ) on\nS is at most 8/γ 2.\nProof: The argument for this new algorithm follows the same lines as the argument for\nthe original Perceptron algorithm.\nAs before, each update increases wt · w∗ by at least γ . What is now a little more complicated\nis to bound the increase in ||wt||, since we are updating on some examples where the angle\nis more than 90 o. For the original algorithm, we had: ||wt+1||2 ≤ || wt||2 + 1, which implies\n||wt+1|| ≤ || wt|| + 1\n2||wt||.\nFor the new algorithm, we instead get\n||wt+1|| ≤ || wt|| + 1\n2||wt|| + γ\n2 ,\nwhich we can see by breaking each x into its orthogonal part (for which the original statement\nholds) and its parallel part (which adds at most γ/ 2 to the length of wt).\nWe can now solve this directly, but just to get a simple upper b ound, just notice that if\n||wt|| ≥ 2/γ then ||wt+1|| ≤ || wt|| + 3γ/ 4. So, after M updates we have:\n||wM +1|| ≤ 2/γ + 3M γ/ 4.\nSolving M γ ≤ 2/γ + 3M γ/ 4 we get M ≤ 8/γ 2, as desired.\nFor more information on perceptron and the analyses given he re, see [Blo62, Nov62, MP69,\nFS99, SSS05, TST05, BB06].\nL2 margins and L1 margins. We saw that Perceptron makes at most 1 /γ 2 mistakes where\nγ is the margin after normalizing by the L2 length of the target and the L2 length of the\nexamples. Winnow makes O((1/γ 2) log n) mistakes after normalizing by the L1 length of\nthe target and L∞ length of the examples. If examples are in {0, 1}n, the nice thing about\nWinnow is that adding extra irrelevant variables (variable s where the target has zero weight)\ndoesn’t aﬀect the L1 − L∞ margin. In general, Winnow does better if examples are dense\nbut the target is sparse, and Perceptron does better if the ta rget is dense but examples are\nsparse.\n2 Kernel functions\nWhat if our data doesn’t have a good linear separator? Here’s a neat idea, called the kernel\ntrick.\n4\nOne thing we might like to do is map our data to a higher dimensi onal space, e.g., look at\nall products of pairs of features, in the hope that data will b e linearly separable there. If\nwe’re lucky, data will be separable by a large margin so we don ’t have to pay a lot in terms\nof mistakes. But this is going to a pain computationally. How ever, it turns out that many\nlearning algorithms only access data through performing do t-products (will get back to how\nto interpret algorithms like Perceptron in this way in a minu te). So, maybe we can perform\nour mapping in such a way that we have an eﬃcient way of computi ng dot-products. This\nleads to idea of a kernel.\nA Kernel is a function K(x, y) such that for some mapping φ , K(x, y) = φ (x) · φ (y).\nSome examples:\n• K(x, y) = (1 + x · y)d.\n• K(x, y) = (1 + x1y1)(1 + x2y2)... (1 + xnyn)\n[corresponds to mapping x, y to list of all products of subsets]\n• String kernels [count how many substrings of length p two strings have in common]\nMore generally, this is nice for the case where examples aren ’t so easy to map directly into\nRn, but we have a reasonable notion of similarity we can encode i n a kernel K.\nNeat fact: many of the learning algorithms for learning line ar separators can be run using\nkernels. E.g., for the Perceptron algorithm, wt is a weighted sum of examples, for all t. I.e.,\nwt = ℓ(xi1)xi1 + · · · + ℓ(xik)xit−1,\nwhere xi1, ..., xit−1 are the examples where we’ve made mistakes so far. So to compu te\nφ (wt) · φ (x), just do:\nℓ(xi1)K(xi1, x) + · · · + ℓ(xit−1)K(xit−1, x).\nThe examples that the hypothesis is written in terms of are ca lled support vectors . If we\nﬁnd the maximum margin separator for a given dataset, that is also something that can be\nwritten in terms of support vectors (not hard to see). That’s the reason for the name “support\nvector machines” for the algorithm that takes a set of data an d ﬁnds the maximum-margin\nseparator.\nReferences\n[BB06] M.F. Balcan and A. Blum. On a theory of learning with si milarity functions. In Proc.\nInternational Conference on Machine Learning (ICML) , pages 73–80, 2006.\n[Blo62] H.D. Block. The perceptron: A model for brain functi oning. Reviews of Modern Physics ,\n34:123–135, 1962. Reprinted in Neurocomputing, Anderson and Rosenfeld.\n[FS99] Y. Freund and R.E. Schapire. Large margin classiﬁcat ion using the Perceptron algorithm.\nMachine Learning, 37(3):277–296, 1999.\n5\n[MP69] M. Minsky and S. Papert. Perceptrons: An Introduction to Computational Geometry .\nThe MIT Press, 1969.\n[Nov62] A.B.J. Novikoﬀ. On convergence proofs on perceptro ns. In Proceedings of the Symposium\non the Mathematical Theory of Automata, Vol. XII , pages 615–622, 1962.\n[SSS05] S. Shalev-Shwartz and Y. Singer. A new perspective o n an old perceptron algorithm. In\nProc. 16thh Annual Conference on Learning Theory , 2005.\n[TST05] P. Tsampouka and J. Shawe-Taylor. Analysis of gener ic perceptron-like large margin\nclassiﬁers. In ECML, 2005.\n6",
  "metadata": {
    "status": "initialized"
  }
}